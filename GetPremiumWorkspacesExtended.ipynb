{"cells":[{"cell_type":"markdown","source":["This code fetches all workspaces and their datasets from the Power BI admin API in batches, adds the workspace ID to each dataset, and creates Spark DataFrames for the workspaces and datasets."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cc759af6-3497-4b59-8d46-72b05da53b50"},{"cell_type":"code","source":["#########################################################################################\n","# Install modules etc.\n","#########################################################################################\n","\n","import requests\n","import json\n","\n","#########################################################################################\n","# Read secrets from Azure Key Vault\n","#########################################################################################\n","key_vault = \"\"\n","tenant_id = mssparkutils.credentials.getSecret(key_vault , \"FabricTenantId\")\n","client_id = mssparkutils.credentials.getSecret(key_vault , \"AdminApiClientId\")\n","client_secret = mssparkutils.credentials.getSecret(key_vault , \"AdminApiClientSecret\")\n","\n","#########################################################################################\n","# Authentication - Replace string variables with your relevant values       \n","#########################################################################################\n","\n","try:\n","    from azure.identity import ClientSecretCredential\n","except Exception:\n","    !pip install azure.identity\n","    from azure.identity import ClientSecretCredential\n","from pyspark.sql import SparkSession\n","\n","#########################################################################################\n","#Fetch all workspaces in premium capacities\n","#########################################################################################\n","# Define the power bi admin api endpoint\n","\n","powerbi_url = \"https://api.powerbi.com/v1.0/myorg/admin/groups\"\n","api_gateway_uri = \"https://api.powerbi.com\"\n","# Define the scope for power bi service\n","scope = \"https://analysis.windows.net/powerbi/api/.default\"\n","\n","# Define the oauth2 token endpoint\n","token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n","\n","# Define the payload for requesting the access token\n","payload = {\n","    \"grant_type\": \"client_credentials\",\n","    \"client_id\": client_id,\n","    \"client_secret\": client_secret,\n","    \"scope\": scope\n","}\n","\n","\n","# Make a post request to get the access token\n","response = requests.post(token_url, data=payload)\n","\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    # Get the access token from the response\n","    access_token = response.json()[\"access_token\"]\n","    headers = {\n","        \"Authorization\": f\"Bearer {access_token}\"\n","    }\n","\n","# Initialize empty lists for workspaces, datasets, dashboards, reports, users and dataflows\n","workspaces = []\n","datasets = []\n","dashboards = []\n","reports = []\n","users = []\n","dataflows = []\n","\n","# Initialize batch and batch_size for pagination\n","batch = 0\n","batch_size = 1000\n","\n","# Start a loop to fetch all workspaces in batches\n","while True:\n","    # Calculate the number of workspaces to skip based on the current batch\n","    skip = batch * batch_size\n","\n","    # Make a GET request to fetch a batch of workspaces\n","    workspace_batch = requests.get(\"https://api.powerbi.com/v1.0/myorg/admin/groups?%24top={}&%24expand=reports%2C%20dashboards%2C%20datasets%2C%20users%2C%20dataflows&%24skip={}&%24filter=isOnDedicatedCapacity%20eq%20true\".format(batch_size,skip), headers=headers)\n","\n","    # Parse the response JSON\n","    workspace_data = workspace_batch.json()\n","\n","    # Get the total number of workspaces\n","    total_workspaces = workspace_data.get(\"@odata.count\", 0)\n","\n","    # If the current batch of workspaces is not empty\n","    if len(workspace_data.get(\"value\", [])) > 0:\n","        # Add the workspaces to the workspaces list\n","        workspaces.extend(workspace_data[\"value\"])\n","\n","        # For each workspace, extract its datasets and add them to the datasets list\n","        for workspace in workspace_data[\"value\"]:\n","            if len(workspace.get(\"datasets\", [])) > 0:\n","                for dataset in workspace[\"datasets\"]:\n","                    # Add workspaceId to each dataset\n","                    dataset[\"workspaceId\"] = workspace[\"id\"]\n","                # Add the datasets to the datasets list\n","                datasets.extend(workspace[\"datasets\"])\n","        \n","        # For each workspace, extract its reports and add them to the reports list\n","        for workspace in workspace_data[\"value\"]:\n","            if len(workspace.get(\"reports\", [])) > 0:\n","                for report in workspace[\"reports\"]:\n","                    # Add workspaceId to each report\n","                    report[\"workspaceId\"] = workspace[\"id\"]\n","                # Add the reports to the reports list\n","                reports.extend(workspace[\"reports\"])\n","\n","        # Extract dashboards from workspace objects\n","        for workspace in workspace_data[\"value\"]:\n","            if len(workspace.get(\"dashboards\", [])) > 0:\n","                for dashboard in workspace[\"dashboards\"]:\n","                    dashboard[\"workspaceId\"] = workspace[\"id\"]\n","                dashboards.extend(workspace[\"dashboards\"])  \n","\n","        # Extract users from workspace objects\n","        for workspace in workspace_data[\"value\"]:\n","            if len(workspace.get(\"users\", [])) > 0:\n","                for user in workspace[\"users\"]:\n","                    user[\"workspaceId\"] = workspace[\"id\"]\n","                users.extend(workspace[\"users\"])\n","        \n","        # Extract dataflows from workspace objects\n","        for workspace in workspace_data[\"value\"]:\n","            if len(workspace.get(\"dataflows\", [])) > 0:\n","                for dataflow in workspace[\"dataflows\"]:\n","                    dataflow[\"workspaceId\"] = workspace[\"id\"]\n","                dataflows.extend(workspace[\"dataflows\"])     \n","\n","\n","    # Move to the next batch\n","    batch += 1\n","\n","    # If we have fetched all workspaces, break the loop\n","    if skip >= total_workspaces:\n","        break\n","\n","#Create spark dataframes and define schema\n","df_workspaces = spark.createDataFrame(workspaces, schema=\"id string, isReadOnly string, dataflowStorageId string, description string, pipelineId string, isOnDedicatedCapacity string, capacityId string, capacityMigrationStatus string, defaultDatasetStorageFormat string, type string, state string, hasWorkspaceLevelSettings  string, name string\")\n","df_datasets = spark.createDataFrame(datasets, schema=\"workspaceId string, id string, name string, addRowsAPIEnabled string,configuredBy string, isRefreshable string, isEffectiveIdentityRequired string, isEffectiveIdentityRolesRequired string, isOnPremGatewayRequired string, targetStorageMode string, contentProviderType string, createdDate string\")\n","df_reports = spark.createDataFrame(reports, schema=\"workspaceId string, appId string, createdBy string, createdDateTime string, description string, embedUrl string, id string, modifiedBy string, modifiedDateTime string, reportType string, originalReportId string, webUrl string, name string, datasetId string\")\n","df_dashboards = spark.createDataFrame(dashboards, schema=\"workspaceId string, id string, displayName string, isReadOnly string\")\n","df_users = spark.createDataFrame(users,  schema=\"workspaceId string, emailAddress string, groupUserAccessRight string, identifier string, principalType string, displayName string\")\n","df_dataflows = spark.createDataFrame(dataflows,  schema=\"workspaceId string, name string, description string, configuredBy string, modelUrl string\")\n","\n","# Print the total number of workspaces, reports, dashboards, and datasets for testing purposes\n","print(\"Total workspaces:\", len(workspaces))\n","print(\"Total reports:\", len(reports))\n","print(\"Total dashboards:\", len(dashboards))\n","print(\"Total datasets:\", len(datasets))\n","print(\"Total users:\", len(users))\n","print(\"Total dataflows:\", len(dataflows))\n","\n","# Write to table\n","df_workspaces.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Workspaces\")\n","df_datasets.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Datasets\")\n","df_reports.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Reports\")\n","df_dashboards.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Dashboards\")\n","df_users.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"WorkspaceUsers\")\n","df_dataflows.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Dataflows\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"20c579c5-3e9e-41bd-b4f4-93b0b4630594"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"f4bb357d-b2b8-465e-a48a-68d8fe1f6118","default_lakehouse_name":"POC_Lakehouse","default_lakehouse_workspace_id":"9faa5399-34ab-4ef6-8dcb-2423f57b2416"}}},"nbformat":4,"nbformat_minor":5}